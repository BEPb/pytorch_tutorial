"""
Python 3.10 программа для изучения градиента Pytorch
Название файла 03_gradient.py

Version: 0.1
Author: Andrej Marinchenko
Date: 2022-05-02
"""

import torch  # библиотека pytorch
from torch.autograd import Variable  # импортировать переменную из библиотеки pytorch

'''Создадим переменную из простого тензора:'''
x = Variable(torch.ones(2, 2) * 2, requires_grad=True)
print('Переменная х - это тензор в котором подключена функция обучения градиента \n', x)

'''В объявлении переменной используется двойной тензор 2x2 и дополнительно указывается, что переменной нужен 
градиент. При использовании этой переменной в нейронных сетях она становится способной к обучению. Если последний 
параметр равен False, переменную нельзя использовать для обучения. В этом простом примере мы не будем ничего 
тренировать, но мы хотим запросить градиент для этой переменной.'''

'''Далее давайте создадим новую переменную на основе x.'''
z = 2 * (x * x) + 5 * x
print('\nПеременная z - это тензор получаемый из зависимости от другого линейного тензора х, в котором подключена '
      'функция обучения градиента \n', z)

'''
Чтобы вычислить градиент этой операции по x, dz/dx, мы можем аналитически получить 4x + 5.
Если все элементы x двойки, то градиент dz/dx является тензором размерности (2,2)
заполнен цифрами 13. Однако сначала нужно запустить обратную операцию .backwards()
для вычисления градиента относительно чего-либо. В нашем случае инициализируется единичный тензор (2,2),
относительно которого мы вычисляем градиент. В этом случае расчет представляет собой просто операцию d/dx:
'''

z.backward(torch.ones(2, 2))
print('\n Результат следующий: \n', x.grad)

'''
Обратите внимание, что это именно то, что мы предсказывали в начале. Обратите внимание, что градиент хранится в 
переменной x свойства .grad. 

Переменные накапливают градиенты.
Мы будем использовать pytorch в нейронной сети. И, как вы знаете, в нейронной сети у нас есть обратное 
распространение, где вычисляются градиенты. Поэтому нам нужно обрабатывать градиенты. 
Разница между переменными и тензором заключается в том, что переменная накапливает градиенты.
Мы также можем выполнять математические операции с переменными.
Для обратного распространения нам нужны переменные
Еще один пример:

Предположим, у нас есть уравнение y = x^2
Определите переменную x = [2, 4] 
После вычислений находим, что y = [4, 16] (y = x^2)
Резюмируем уравнение: o = (1/2)sum(y) = (1/2)sum(x^2)
производная от о = х
Результат равен x, поэтому градиенты [2,4]
Позволяет реализовать:
'''

# давайте сделаем базовое обратное распространение, у нас есть уравнение, которое y = x^2
array = [2, 4]
tensor = torch.Tensor(array)
x = Variable(tensor, requires_grad = True)
print("\n x =  ", x)
y = x**2
print("\n y =  ", y)

# резюмируем уравнение o = 1/2*sum(y)
o = (1/2)*sum(y)
print("\n o =  ", o)

# обратное распространение
o.backward()  # вычисление градиентов

# Как я определил, переменные накапливают градиенты. В этой части есть только одна переменная x.
# Поэтому переменная x должна иметь градиенты
# Давайте посмотрим на градиенты с помощью x.grad
print("Градиенты: ", x.grad)


