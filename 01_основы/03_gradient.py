"""
Python 3.10 программа для изучения градиента Pytorch
Название файла 03_gradient.py

Version: 0.1
Author: Andrej Marinchenko
Date: 2022-05-02
"""

import torch  # библиотека pytorch
from torch.autograd import Variable  # импортировать переменную из библиотеки pytorch
import torch.nn as nn

# ================================================================== #
#                         Оглавление                                 #
# ================================================================== #

# 1. Пример 1 (переменная)                        (Line 21 to 41)
# 2. Пример 2 (вычисление градиентов)             (Line 43 to 97)
# 3. Пример 3 (частичное преобразование)          (Line 99 to 122)


# ================================================================== #
#                     1. Пример 1 (переменная)                       #
# ================================================================== #



'''Создадим переменную из простого тензора:'''
x = Variable(torch.ones(2, 2) * 2, requires_grad=True)
print('Переменная х - это тензор в котором подключена функция обучения градиента \n', x)

'''В объявлении переменной используется двойной тензор 2x2 и дополнительно указывается, что переменной нужен 
градиент. При использовании этой переменной в нейронных сетях она становится способной к обучению. Если последний 
параметр равен False, переменную нельзя использовать для обучения. В этом простом примере мы не будем ничего 
тренировать, но мы хотим запросить градиент для этой переменной.'''

'''Далее давайте создадим новую переменную на основе x.'''
z = 2 * (x * x) + 5 * x
print('\nПеременная z - это тензор получаемый из зависимости от другого линейного тензора х, в котором подключена '
      'функция обучения градиента \n', z)

# ================================================================== #
#                     2. Пример 2 (вычисление градиентов)            #
# ================================================================== #

'''
Чтобы вычислить градиент этой операции по x, dz/dx, мы можем аналитически получить 4x + 5.
Если все элементы x двойки, то градиент dz/dx является тензором размерности (2,2)
заполнен цифрами 13. Однако сначала нужно запустить обратную операцию .backwards()
для вычисления градиента относительно чего-либо. В нашем случае инициализируется единичный тензор (2,2),
относительно которого мы вычисляем градиент. В этом случае расчет представляет собой просто операцию d/dx:
'''

z.backward(torch.ones(2, 2))
print('\n Результат следующий: \n', x.grad)

'''
Обратите внимание, что это именно то, что мы предсказывали в начале. Обратите внимание, что градиент хранится в 
переменной x свойства .grad. 

Переменные накапливают градиенты.
Мы будем использовать pytorch в нейронной сети. И, как вы знаете, в нейронной сети у нас есть обратное 
распространение, где вычисляются градиенты. Поэтому нам нужно обрабатывать градиенты. 
Разница между переменными и тензором заключается в том, что переменная накапливает градиенты.
Мы также можем выполнять математические операции с переменными.
Для обратного распространения нам нужны переменные
Еще один пример:

Предположим, у нас есть уравнение y = x^2
Определите переменную x = [2, 4] 
После вычислений находим, что y = [4, 16] (y = x^2)
Резюмируем уравнение: o = (1/2)sum(y) = (1/2)sum(x^2)
производная от о = х
Результат равен x, поэтому градиенты [2,4]
Позволяет реализовать:
'''

# давайте сделаем базовое обратное распространение, у нас есть уравнение, которое y = x^2
array = [2, 4]
tensor = torch.Tensor(array)
x = Variable(tensor, requires_grad = True)
print("\n x =  ", x)
y = x**2
print("\n y =  ", y)

# резюмируем уравнение o = 1/2*sum(y)
o = (1/2)*sum(y)
print("\n o =  ", o)

# обратное распространение
o.backward()  # вычисление градиентов

# Как я определил, переменные накапливают градиенты. В этой части есть только одна переменная x.
# Поэтому переменная x должна иметь градиенты
# Давайте посмотрим на градиенты с помощью x.grad
print("Градиенты: ", x.grad)

# ================================================================== #
#                     3. Пример 3 (вычисление градиентов)            #
# ================================================================== #


# Создание тензоров из одного числа
x = torch.tensor(1., requires_grad=True)
y = torch.tensor(2., requires_grad=True)
z = torch.tensor(3., requires_grad=True)

print('\n переменная х ', x)
print('\n переменная y ', y)
print('\n переменная z ', z)

# Построим расчетный граф
w = x * y + z    # y = 2 * 1 + 3

# Вычисление градиентов
w.backward()

# Посмотрим градиенты
print('\n градиент х ', x.grad)    # x.grad = 2
print('\n градиент y ', y.grad)    # y.grad = 1
print('\n градиент z ', z.grad)    # z.grad = 1

# ================================================================== #
#                    4. Пример 4 (Базовый автоградиент)              #
# ================================================================== #

# Создадим случайные тензоры размерности (10, 3) и (10, 2).
x = torch.randn(10, 3)
y = torch.randn(10, 2)

# Создадим полносвязный слой.
linear = nn.Linear(3, 2)
print('\n веса линейной модели: ', linear.weight)
print('\n линейное смещение: ', linear.bias)

# Создадим функцию потерь и оптимизатор
criterion = nn.MSELoss()
optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)

# Проход вперёд.
pred = linear(x)

# Вычисляем ошибку
loss = criterion(pred, y)
print('\n ошибка: ', loss.item())

# обратное распространение ошибки
loss.backward()

# выведем градиенты
print('\n градиент dL/dw: ', linear.weight.grad)
print('\n градиент dL/db: ', linear.bias.grad)

# 1-шаговый градиентный спуск.
optimizer.step()

# Вы также можете выполнить градиентный спуск на низком уровне.
# linear.weight.data.sub_(0.01 * linear.weight.grad.data)
# linear.bias.data.sub_(0.01 * linear.bias.grad.data)


# Распечатайте потери после 1-ступенчатого градиентного спуска.
pred = linear(x)
loss = criterion(pred, y)
print('\n ошибка после 1-го шага оптимизации: ', loss.item())