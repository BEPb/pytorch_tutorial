"""
Python 3.10 программа для изучения простой нейронной сети Pytorch
Название файла 04_simple_neural_network.py

Version: 0.1
Author: Andrej Marinchenko
Date: 2022-05-02
"""


'''Входной слой состоит из 8 нейронов, которые составляют входные данные в наборе данных. Затем входные данные 
передаются через два скрытых слоя, каждый из которых содержит 512 узлов, с использованием функции активации 
линейного выпрямителя (ReLU). Наконец, у нас есть выходной слой с двумя узлами, соответствующими результату.  Для 
такой задачи классификации мы будем использовать выходной слой softmax.   

### Класс для построения нейронной сети
Для создания нейросети в PyTorch используется класс nn.Module. Для его использования необходимо наследование, 
которое позволит использовать весь функционал базового класса nn.Module, но при этом еще возможно переписать базовый 
класс для построения модели или прямого прохождения по сети. Код ниже поможет объяснить это:'''

'''
Основная структура данных torch.nn — это модуль, представляющий собой абстрактное понятие, которое может представлять определенный слой в нейронной сети.
или нейронная сеть, содержащая много слоев. На практике наиболее распространенным способом является наследование nn.Module и создание собственной сети/уровня.
Давайте сначала посмотрим, как использовать nn.Module для реализации вашего собственного полносвязного уровня. Полносвязный слой, также известный как аффинный слой.
'''

import torch.nn as nn  # библиотека нейронной сети pytorch
import torch.nn.functional as F  # функции нейронной сети


class Net(nn.Module):  # В таком определении можно увидеть наследование базового класса nn.Module
    def __init__(self):  # инициализация класса или конструктор класса
        super(Net, self).__init__()  # функция super() создает объект базового класса

        # в следующих трех строках кода мы создаем полносвязные слои
        '''
        Полносвязный слой нейронной сети представлен объектом nn.Linear,
         в котором первым аргументом является количество узлов в i-м слое, а вторым — количество узлов в слое i+1. 
         Как видно из кода, первый слой принимает 7 узлов в качестве входных данных и подключается к первому скрытому 
         слою с 512 узлами.
        '''
        self.fc1 = nn.Linear(7, 512)

        # Далее идет подключение к другому скрытому слою с 512 узлами.
        self.fc2 = nn.Linear(512, 512)

        # И, наконец, подключение последнего скрытого слоя к выходному слою двумя узлами.
        self.fc3 = nn.Linear(512, 2)

        # Определим пропорцию или нейроны для отсева, отброса или обнуления (dropout)
        self.dropout = nn.Dropout(0.2)  # 0,2 - вероятность обнуления элемента. По умолчанию: 0,5

        '''
        После определения скелета сетевой архитектуры необходимо задать
         принципы, по которым данные будут проходить через него. Это делается с помощью метода forward().
         определяется, что переопределяет фиктивный метод в базовом классе и требует определения для каждой сети
        '''

    def forward(self, x):
        '''Для метода forward() мы принимаем входные данные x в качестве основного аргумента
         Далее загружаем все в первый полносвязный слой self.fc1(x) и применяем активацию ReLU
         функция для узлов в этом слое с помощью F.relu()'''
        x = F.relu(self.fc1(x))
        x = self.dropout(x)

         # Из-за иерархической природы этой нейронной сети мы заменяем x на каждом этапе и отправляем
         # это на следующий слой
        x = F.relu(self.fc2(x))
        x = self.dropout(x)

        # Проделываем эту процедуру на трех связанных слоях, кроме последнего.
        x = self.fc3(x)

        '''На последнем слое возвращаем не ReLU, а логарифмическую функцию активации softmax.
         Это, в сочетании с отрицательной функцией потерь логарифмического правдоподобия, дает мультиклассовая
         функция потерь на основе кросс-энтропии, которую мы будем использовать для обучения сети.
        '''
        return x  # получаем бинарное предсказание

'''Мы определили нейронную сеть. Следующим шагом является создание экземпляра этой архитектуры.:'''

model = Net()
print('При выводе экземпляра класса Net получаем следующее: \n', model)
print('Что очень удобно, так как подтверждает структуру нашей нейросети.')

'''Обучение сети¶
Далее необходимо указать метод оптимизации и критерий качества:'''

import torch.optim as optim  # это пакет, реализующий различные алгоритмы оптимизации.
# Наиболее часто используемые методы уже поддерживаются, а интерфейс достаточно общий,
# чтобы в будущем можно было легко интегрировать более сложные

learning_rate = 0.01
# В первой строке мы создаем оптимизатор на основе стохастического градиентного спуска,
# установка скорости обучения (в нашем случае мы определим этот показатель равным 0,01)

# Выполняем оптимизацию стохастическим градиентным спуском
# Еще в оптимизаторе нужно определить все остальные сетевые параметры,
# но это легко делается в PyTorch благодаря методу .parameters()
# в базовом классе nn.Module, который наследуется от него в новый класс Net

optimizer = optim.SGD(model.parameters(), lr=learning_rate)
# optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)

'''
Затем устанавливается метрика контроля качества, отрицательная функция потерь логарифмического правдоподобия.
Этот тип функции в сочетании с логарифмической функцией softmax на выходе
нейронной сети, дает эквивалентную кросс-энтропийную потерю классификации.
'''

# error = nn.NLLLoss()
error = nn.CrossEntropyLoss()  # Функция потерь (перекрестная энтропия CE)

'''Внешний обучающий цикл проходит через количество эпох, а внутренний обучающий цикл проходит через все обучающие 
данные в пакетах, размер которых задается в коде как batch_size. В следующей строке мы конвертируем данные и целевую 
переменную в переменные PyTorch. Входной набор данных имеет размер (batch_size, 1, 28, 28) при извлечении из 
загрузчика данных. Такой 4D-тензор больше подходит для архитектуры сверточной нейронной сети, чем для нашей 
полносвязной сети. Однако необходимо уменьшить размерность данных с (1,28,28) до одномерного случая 
для 28 x 28 = 784 входных узлов.'''