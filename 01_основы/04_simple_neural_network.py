"""
Python 3.10 программа для изучения простой нейронной сети Pytorch
Название файла 04_simple_neural_network.py

Version: 0.1
Author: Andrej Marinchenko
Date: 2022-05-02
"""


'''Входной слой состоит из 8 нейронов, которые составляют входные данные в наборе данных. Затем входные данные 
передаются через два скрытых слоя, каждый из которых содержит 512 узлов, с использованием функции активации 
линейного выпрямителя (ReLU). Наконец, у нас есть выходной слой с двумя узлами, соответствующими результату.  Для 
такой задачи классификации мы будем использовать выходной слой softmax.   

### Класс для построения нейронной сети
Для создания нейросети в PyTorch используется класс nn.Module. Для его использования необходимо наследование, 
которое позволит использовать весь функционал базового класса nn.Module, но при этом еще возможно переписать базовый 
класс для построения модели или прямого прохождения по сети. Код ниже поможет объяснить это:'''

'''
The main data structure torch.nn is a module, which is an abstract concept that can represent a specific layer in a neural network, 
or a neural network containing many layers. In practice the most common way is to inherit nn.Module and write your own network/layer. 
Let's first see how to use nn.Module to implement your own fully connected layer. A fully connected layer, also known as an affine layer
'''

import torch.nn as nn  # библиотека нейронной сети pytorch
import torch.nn.functional as F  # функции нейронной сети


class Net(nn.Module):  # В таком определении можно увидеть наследование базового класса nn.Module
    def __init__(self):  # инициализация класса или конструктор класса
        super(Net, self).__init__()  # функция super() создает объект базового класса

        # в следующих трех строках кода мы создаем полносвязные слои
        '''
        Полносвязный слой нейронной сети представлен объектом nn.Linear,
         в котором первым аргументом является количество узлов в i-м слое, а вторым — количество узлов в слое i+1. 
         Как видно из кода, первый слой принимает 7 узлов в качестве входных данных и подключается к первому скрытому 
         слою с 512 узлами.
        '''
        self.fc1 = nn.Linear(7, 512)

        # Далее идет подключение к другому скрытому слою с 512 узлами.
        self.fc2 = nn.Linear(512, 512)

        # И, наконец, подключение последнего скрытого слоя к выходному слою двумя узлами.
        self.fc3 = nn.Linear(512, 2)

        # Определим пропорцию или нейроны для отсева, отброса или обнуления (dropout)
        self.dropout = nn.Dropout(0.2)  # 0,2 - вероятность обнуления элемента. По умолчанию: 0,5

        '''
        После определения скелета сетевой архитектуры необходимо задать
         принципы, по которым данные будут проходить через него. Это делается с помощью метода forward().
         определяется, что переопределяет фиктивный метод в базовом классе и требует определения для каждой сети
        '''

    def forward(self, x):
        '''Для метода forward() мы принимаем входные данные x в качестве основного аргумента
         Далее загружаем все в первый полносвязный слой self.fc1(x) и применяем активацию ReLU
         функция для узлов в этом слое с помощью F.relu()'''
        x = F.relu(self.fc1(x))
        x = self.dropout(x)

         # Из-за иерархической природы этой нейронной сети мы заменяем x на каждом этапе и отправляем
         # это на следующий слой
        x = F.relu(self.fc2(x))
        x = self.dropout(x)

        # Проделываем эту процедуру на трех связанных слоях, кроме последнего.
        x = self.fc3(x)

        '''На последнем слое возвращаем не ReLU, а логарифмическую функцию активации softmax.
         Это, в сочетании с отрицательной функцией потерь логарифмического правдоподобия, дает мультиклассовая
         функция потерь на основе кросс-энтропии, которую мы будем использовать для обучения сети.
        '''
        return x  # получаем бинарное предсказание
